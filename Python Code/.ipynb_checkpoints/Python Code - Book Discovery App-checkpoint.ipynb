{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7f240a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries from NLTK to Tokenize a sentence\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import libraries and punctuation data\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "# Import libraries for Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# POS Tagging\n",
    "from nltk.tag import pos_tag\n",
    "import stanza\n",
    "from pattern.text.en import singularize\n",
    "from SPARQLWrapper import SPARQLWrapper2\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b87d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to normalise\n",
    "\n",
    "def text_normalization(text):\n",
    "    processed_text =  re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)\n",
    "    processed_text = \" \".join(processed_text.split())\n",
    "    return processed_text\n",
    "\n",
    "# Method to lammatize\n",
    "\n",
    "def text_lemmatization(processed_text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(processed_text)\n",
    "    required_words = [wordnet_lemmatizer.lemmatize(x, 'v') for x in  tokens]\n",
    "    sentence_with_lemmnatized_word = \" \".join(required_words)\n",
    "    return sentence_with_lemmnatized_word\n",
    "\n",
    "# Method to process text\n",
    "\n",
    "def text_processing(text):\n",
    "    text = text_normalization(text)\n",
    "    text = text_lemmatization(text)\n",
    "    return text\n",
    "\n",
    "# Method for POS tagging\n",
    "\n",
    "def pos_tags_creation(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "81362077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Stanza to recognise Entities\n",
    "\n",
    "def fetch_named_entities(text):\n",
    "    nlp = stanza.Pipeline('en', download_method=stanza.DownloadMethod.NONE)\n",
    "    results = nlp(text)\n",
    "    return results.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "95453cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of keywords\n",
    "\n",
    "properties_list = [['has_genre','has genres', 'with genre','genre'], \n",
    "             ['written_by','written by', 'by author', 'author'], \n",
    "             ['receive_award', 'receive award', 'received', 'receive', 'awarded', 'award'], \n",
    "             ['first_published_country', 'first published country', 'first', 'published', 'country'], \n",
    "             ['has_category', 'has category', 'category']]\n",
    "\n",
    "classes_list = [['book', 'books'], \n",
    "              ['author', 'authors', 'writer'], \n",
    "              ['person', 'people'], \n",
    "              ['award', 'awards'], \n",
    "              ['country', 'countries'], \n",
    "              ['category', 'categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9b827da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the bag of keywords\n",
    "\n",
    "def property_mappings(word, properties_list):\n",
    "    mapped_property = \"\"\n",
    "    index = [[i, prop.index(word)]\n",
    "             for i, prop in enumerate(properties_list)\n",
    "             if word in prop]\n",
    "    if len(index) > 0:\n",
    "        mapped_property = properties_list[index[0][0]][0]\n",
    "    return mapped_property\n",
    "\n",
    "def classes_mappings(word, classes_list):\n",
    "    mapped_class = \"\"\n",
    "    index = [[i, cls.index(word)]\n",
    "             for i, cls in enumerate(classes_list)\n",
    "             if word in cls]\n",
    "    if len(index) > 0:\n",
    "        mapped_class = classes_list[index[0][0]][0]\n",
    "    return mapped_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c1383139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries\n",
    "\n",
    "text = \"all books with Thriller genre\"\n",
    "# text = \"all books by author Chetan Bhagat\"\n",
    "# text = \"all books awarded with Goodreads Choice Awards\"\n",
    "# text = \"all books with published country as India\"\n",
    "# text = \"all books Top Rated category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "00c99e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('all', 'DT'),\n",
       " ('book', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('Thriller', 'NNP'),\n",
       " ('genre', 'NN')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process User Text\n",
    "\n",
    "processed_text = text_processing(text)\n",
    "tags = pos_tags_creation(processed_text)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7eac74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Classes and Properties\n",
    "\n",
    "def create_classes_properties(tagged_chuncks, classes, properties, classes_list, properties_list):\n",
    "    for entity in tagged_chuncks: \n",
    "        if hasattr(entity, \"label\"): \n",
    "             print(\"check entity: \",entity)\n",
    "        else:\n",
    "            if entity[1] == \"NNS\":\n",
    "                mapped_class = classes_mappings(singularize(entity[0]), classes_list)\n",
    "                if mapped_class!=\"\":\n",
    "                    classes.append(mapped_class.capitalize())\n",
    "            elif entity[1] == \"VBP\":\n",
    "                mapped_property = property_mappings(entity[0], properties_list)\n",
    "                if mapped_property!=\"\": \n",
    "                    properties.append(mapped_property)\n",
    "\n",
    "            elif entity[1] == \"NN\":\n",
    "                mapped_property = property_mappings(entity[0], properties_list)\n",
    "                mapped_class = classes_mappings(singularize(entity[0]), classes_list)\n",
    "                if mapped_property!=\"\": \n",
    "                    properties.append(mapped_property)\n",
    "                elif mapped_class!=\"\":\n",
    "                    classes.append(mapped_class.capitalize())\n",
    "    return classes, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "37a9f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Individuals\n",
    "\n",
    "def create_individuals(text, individuals, classes_list):\n",
    "    named_entities = fetch_named_entities(text)\n",
    "    for e in named_entities:\n",
    "        print(e)\n",
    "        mapped_ind_class = classes_mappings(singularize(e.type.lower()), classes_list)\n",
    "        if mapped_ind_class!=\"\":\n",
    "            individuals[e.text] = mapped_ind_class.capitalize()\n",
    "        else:\n",
    "            individuals[e.text] = \"not_mapped\"\n",
    "    return individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3cc8e380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 20:38:19 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2023-11-26 20:38:19 INFO: Using device: cpu\n",
      "2023-11-26 20:38:19 INFO: Loading: tokenize\n",
      "2023-11-26 20:38:19 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check entity:  (PERSON Thriller/NNP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 20:38:19 INFO: Loading: lemma\n",
      "2023-11-26 20:38:19 INFO: Loading: constituency\n",
      "2023-11-26 20:38:21 INFO: Loading: depparse\n",
      "2023-11-26 20:38:22 INFO: Loading: sentiment\n",
      "2023-11-26 20:38:22 INFO: Loading: ner\n",
      "2023-11-26 20:38:23 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Thriller\",\n",
      "  \"type\": \"WORK_OF_ART\",\n",
      "  \"start_char\": 15,\n",
      "  \"end_char\": 23\n",
      "}\n",
      "Classes:  ['Book']\n",
      "Properties:  ['has_genre']\n",
      "Individuals:  {'Thriller': 'not_mapped'}\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the named entities and print their labels\n",
    "\n",
    "tagged_chuncks = nltk.ne_chunk(tags) \n",
    "classes = []\n",
    "properties = []\n",
    "individuals = {}\n",
    "\n",
    "# CREATE CLASSES AND PROPERTIES\n",
    "classes, properties = create_classes_properties(tagged_chuncks, classes, properties, classes_list, properties_list)\n",
    "\n",
    "# CREATE INDIVIDUALS\n",
    "individuals = create_individuals(text, individuals, classes_list)\n",
    "\n",
    "print(\"Classes: \",classes)\n",
    "print(\"Properties: \",properties)\n",
    "print(\"Individuals: \",individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "64f0b242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classes triple\n",
    "\n",
    "def make_classes_triple(classes, individuals, class_triple):\n",
    "    if len(classes)>0:\n",
    "        class_triple = \"{?y a book:\"+classes[0]+\"}\"\n",
    "    if(len(individuals)>0):\n",
    "        class_triple = class_triple + \"{?x (book:|!book:)|^(book:|!book:)* ?y}\"\n",
    "    return class_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a74b15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Properties triple\n",
    "\n",
    "def make_properties_triple(properties, prop_triple):\n",
    "    prop_triple = \"{?x book:\"+properties[0]+\"|^book:\"+properties[0]+\" ?y}\"\n",
    "    return prop_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "efa034a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Individuals triple\n",
    "\n",
    "def make_individual_triple(properties, individuals, individual_triple):\n",
    "    first_key = next(iter(individuals))\n",
    "    if(individuals[first_key]==\"not_mapped\"):\n",
    "        if(len(properties)==0):\n",
    "            individual_triple = \"{?x a ?y} {?y book:name ?bname} {?x book:name ?name} FILTER(?name='\"+first_key+\"')\"            \n",
    "        else:\n",
    "            individual_triple = \"{?y book:name ?bname} {?x book:name ?name} FILTER(?name='\"+first_key+\"')\"\n",
    "    else:\n",
    "         individual_triple = \"{?x a book:\"+individuals[first_key]+\"} {?y book:name ?bname} {?x book:name ?name} FILTER(?name='\"+first_key+\"')\"\n",
    "    return individual_triple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "745a007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SPARQL Query\n",
    "\n",
    "def make_sparql_query(class_triple, prop_triple, individual_triple):\n",
    "    query_start =      \"\"\"\n",
    "                    PREFIX book:<http://www.book-discovery.com/ontologies#>\n",
    "                    SELECT *\n",
    "                    WHERE{\n",
    "                    \"\"\"\n",
    "    query_end =    \"}\"\n",
    "\n",
    "    return query_start+class_triple+prop_triple+individual_triple+query_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c36faa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                    PREFIX book:<http://www.book-discovery.com/ontologies#>\n",
      "                    SELECT *\n",
      "                    WHERE{\n",
      "                    {?y a book:Book}{?x (book:|!book:)|^(book:|!book:)* ?y}{?x book:has_genre|^book:has_genre ?y}{?y book:name ?bname} {?x book:name ?name} FILTER(?name='Thriller')}\n"
     ]
    }
   ],
   "source": [
    "sparql = SPARQLWrapper2(\"http://localhost:3030/BookDiscoveryAppQueries/query\")\n",
    "\n",
    "class_triple = \"\"\n",
    "prop_triple = \"\"\n",
    "individual_triple = \"\"\n",
    "\n",
    "# Create classes triple\n",
    "class_triple = make_classes_triple(classes, individuals, class_triple)\n",
    "\n",
    "# Create Properties triple\n",
    "prop_triple = make_properties_triple(properties, prop_triple)\n",
    "\n",
    "# Create Individuals triple\n",
    "if len(individuals)>0:\n",
    "    individual_triple = make_individual_triple(properties, individuals, individual_triple)\n",
    "\n",
    "\n",
    "query = make_sparql_query(class_triple, prop_triple, individual_triple)\n",
    "print(query)\n",
    "\n",
    "sparql.setQuery(query)\n",
    "results = sparql.query().bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bf965a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_results(table_columns, results):\n",
    "    table_values = [table_columns]\n",
    "    for result in results:\n",
    "        column_values = [result[\"bname\"].value, result[\"name\"].value, result[\"y\"].value]\n",
    "        table_values.append(column_values)\n",
    "    return table_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4c373dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book          Genre     Book URI\n",
      "------------  --------  -----------------------------------------------------\n",
      "Life Of Pi    Thriller  http://www.book-discovery.com/ontologies#life_of_pi\n",
      "Sacred Games  Thriller  http://www.book-discovery.com/ontologies#sacred_games\n",
      "400 days      Thriller  http://www.book-discovery.com/ontologies#400_days\n"
     ]
    }
   ],
   "source": [
    "# Query1: All books with Thriller genre\n",
    "\n",
    "table_columns = ['Book', 'Genre', 'Book URI']\n",
    "table_values = get_query_results(table_columns, results)\n",
    "print(tabulate(table_values, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "833b2eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book             Author         Book URI\n",
      "---------------  -------------  --------------------------------------------------------\n",
      "Revolution 2020  Chetan Bhagat  http://www.book-discovery.com/ontologies#revolution_2020\n",
      "400 days         Chetan Bhagat  http://www.book-discovery.com/ontologies#400_days\n"
     ]
    }
   ],
   "source": [
    "# Query2: All books by author Chetan Bhagat\n",
    "\n",
    "table_columns = ['Book', 'Author', 'Book URI']\n",
    "table_values = get_query_results(table_columns, results)\n",
    "print(tabulate(table_values, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "46d90785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book             Award                    Book URI\n",
      "---------------  -----------------------  --------------------------------------------------------\n",
      "A Promised Land  Goodreads Choice Awards  http://www.book-discovery.com/ontologies#a_promised_land\n",
      "Revolution 2020  Goodreads Choice Awards  http://www.book-discovery.com/ontologies#revolution_2020\n",
      "400 days         Goodreads Choice Awards  http://www.book-discovery.com/ontologies#400_days\n"
     ]
    }
   ],
   "source": [
    "# Query3: All books awarded with Goodreads Choice Awards\n",
    "\n",
    "table_columns = ['Book', 'Award', 'Book URI']\n",
    "table_values = get_query_results(table_columns, results)\n",
    "print(tabulate(table_values, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "aa448677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book             Country    Book URI\n",
      "---------------  ---------  --------------------------------------------------------\n",
      "Revolution 2020  India      http://www.book-discovery.com/ontologies#revolution_2020\n",
      "Sacred Games     India      http://www.book-discovery.com/ontologies#sacred_games\n",
      "400 days         India      http://www.book-discovery.com/ontologies#400_days\n"
     ]
    }
   ],
   "source": [
    "# Query4: All books with published country as India\n",
    "\n",
    "table_columns = ['Book', 'Country', 'Book URI']\n",
    "table_values = get_query_results(table_columns, results)\n",
    "print(tabulate(table_values, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2f40fed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book             Category    Book URI\n",
      "---------------  ----------  --------------------------------------------------------\n",
      "Harry Potter     Top Rated   http://www.book-discovery.com/ontologies#harry_potter\n",
      "Revolution 2020  Top Rated   http://www.book-discovery.com/ontologies#revolution_2020\n",
      "400 days         Top Rated   http://www.book-discovery.com/ontologies#400_days\n"
     ]
    }
   ],
   "source": [
    "# Query5: All books Top Rated category\n",
    "\n",
    "table_columns = ['Book', 'Category', 'Book URI']\n",
    "table_values = get_query_results(table_columns, results)\n",
    "print(tabulate(table_values, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78e013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
