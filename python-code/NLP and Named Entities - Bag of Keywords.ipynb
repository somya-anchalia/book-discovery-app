{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e2aab2",
   "metadata": {},
   "source": [
    "# Import REGEX library and punctuation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c42554be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f658c2",
   "metadata": {},
   "source": [
    "# Import libraries from NLTK to Tokenize a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "624486b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt') # commented since already downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857a57d",
   "metadata": {},
   "source": [
    "# Import libraries for Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1e73bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19c4cb",
   "metadata": {},
   "source": [
    "# Write functions to normalise and lammatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ad89841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    processed_text =  re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)\n",
    "    processed_text = \" \".join(processed_text.split())\n",
    "    return processed_text\n",
    "\n",
    "def lemmatize(processed_text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(processed_text)\n",
    "    required_words = [wordnet_lemmatizer.lemmatize(x, 'v') for x in  tokens]\n",
    "    sentence_with_lemmnatized_word = \" \".join(required_words)\n",
    "    return sentence_with_lemmnatized_word\n",
    "\n",
    "def process_text(text):\n",
    "    text = normalize(text)\n",
    "    text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf67742e",
   "metadata": {},
   "source": [
    "# POS Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b1d9f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ed18cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def create_pos_tags(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3a46e4",
   "metadata": {},
   "source": [
    "# Import Stanza to recognise Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f3d2e",
   "metadata": {},
   "source": [
    "You may need to install stanza if it is not already there using pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35887a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "# stanza.download('en')\n",
    "\n",
    "def get_named_entities(text):\n",
    "    nlp = stanza.Pipeline ('en', download_method=stanza.DownloadMethod.NONE)\n",
    "    results = nlp (text)\n",
    "    return results.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e61820d",
   "metadata": {},
   "source": [
    "# Bag of Keywords Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3af3f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "prop_list = [['has_genre','has genres', 'with genre','genres','genre'], ['has_genre', 'with genre','genres']]\n",
    "class_list = [['book', 'books'],['person', 'people'],['language']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8990b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_property(word, prop_list):\n",
    "    mapped_property = \"\"\n",
    "    index = [[i, prop.index(word)]\n",
    "             for i, prop in enumerate(prop_list)\n",
    "             if word in prop]\n",
    "    if len(index) > 0:\n",
    "        mapped_property = prop_list[index[0][0]][0]\n",
    "    return mapped_property\n",
    "\n",
    "def map_class(word, class_list):\n",
    "    mapped_class = \"\"\n",
    "    index = [[i, cls.index(word)]\n",
    "             for i, cls in enumerate(class_list)\n",
    "             if word in cls]\n",
    "    if len(index) > 0:\n",
    "        mapped_class = class_list[index[0][0]][0]\n",
    "    return mapped_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ee7bd",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b584079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"all of the movies     by Greg Nicotero\"\n",
    "#text = \"all of the movies directed    by Greg Nicotero\"\n",
    "#text = \"list cast of The Walking Dead\"\n",
    "#text = \"movies in English\"\n",
    "#text = \"what is duration of Titanic?\"\n",
    "\n",
    "\n",
    "text = \"all books with Thriller genre\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9de3e173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('all', 'DT'),\n",
       " ('book', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('Thriller', 'NNP'),\n",
       " ('genre', 'NN')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text = process_text(text)\n",
    "tags = create_pos_tags(processed_text)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd889a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 21:20:48 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2023-11-21 21:20:48 INFO: Using device: cpu\n",
      "2023-11-21 21:20:48 INFO: Loading: tokenize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check entity:  (PERSON Thriller/NNP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 21:20:48 INFO: Loading: pos\n",
      "2023-11-21 21:20:49 INFO: Loading: lemma\n",
      "2023-11-21 21:20:49 INFO: Loading: constituency\n",
      "2023-11-21 21:20:50 INFO: Loading: depparse\n",
      "2023-11-21 21:20:51 INFO: Loading: sentiment\n",
      "2023-11-21 21:20:51 INFO: Loading: ner\n",
      "2023-11-21 21:20:52 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Thriller\",\n",
      "  \"type\": \"WORK_OF_ART\",\n",
      "  \"start_char\": 15,\n",
      "  \"end_char\": 23\n",
      "}\n",
      "Classes:  ['Book']\n",
      "Properties:  ['has_genre']\n",
      "Individuals:  {'Thriller': 'not_mapped'}\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#!pip install pattern\n",
    "from pattern.text.en import singularize\n",
    "\n",
    "tagged_chuncks = nltk.ne_chunk(tags) \n",
    "# Iterate over the named entities and print their labels \n",
    "classes = []\n",
    "properties = []\n",
    "individuals = {}\n",
    "\n",
    "for entity in tagged_chuncks: \n",
    "    if hasattr(entity, \"label\"): \n",
    "         print(\"check entity: \",entity)\n",
    "    else:\n",
    "        if entity[1] == \"NNS\":\n",
    "            mapped_class = map_class(singularize(entity[0]), class_list)\n",
    "            if mapped_class!=\"\":\n",
    "                classes.append(mapped_class.capitalize())\n",
    "        elif entity[1] == \"VBP\":\n",
    "            mapped_property = map_property(entity[0], prop_list)\n",
    "            if mapped_property!=\"\": \n",
    "                properties.append(mapped_property)\n",
    "            \n",
    "        elif entity[1] == \"NN\":\n",
    "            mapped_property = map_property(entity[0], prop_list)\n",
    "            mapped_class = map_class(singularize(entity[0]), class_list)\n",
    "            if mapped_property!=\"\": \n",
    "                properties.append(mapped_property)\n",
    "            elif mapped_class!=\"\":\n",
    "                classes.append(mapped_class.capitalize())\n",
    "\n",
    "######################## CREATE INDIVIDUALS ##############################\n",
    "named_entities = get_named_entities(text)\n",
    "\n",
    "for e in named_entities: \n",
    "    print(e)\n",
    "    mapped_ind_class = map_class(singularize(e.type.lower()), class_list)\n",
    "    if mapped_ind_class!=\"\":\n",
    "        individuals[e.text] = mapped_ind_class.capitalize()\n",
    "    else:\n",
    "        individuals[e.text] = \"not_mapped\"\n",
    "        #classes.append(mapped_ind_class.capitalize())\n",
    "\n",
    "print(\"Classes: \",classes)\n",
    "print(\"Properties: \",properties)\n",
    "print(\"Individuals: \",individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e046a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79be7d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                PREFIX book:<http://www.book-discovery.com/ontologies#>\n",
      "                SELECT  ?y\n",
      "                WHERE{\n",
      "                {?y a book:Book}{?x (book:|!book:)|^(book:|!book:)* ?y}{?x book:has_genre|^book:has_genre ?y} {?x book:name ?name} FILTER(?name='Thriller')}\n"
     ]
    }
   ],
   "source": [
    "sparql = SPARQLWrapper2(\"http://localhost:3030/BookDiscoveryApplication/query\")\n",
    "c_triple = \"\"\n",
    "p_triple = \"\"\n",
    "i_triple = \"\"\n",
    "\n",
    "\n",
    "if len(classes)>0:\n",
    "    c_triple = \"{?y a book:\"+classes[0]+\"}\"\n",
    "    if(len(individuals)>0):\n",
    "        c_triple = c_triple + \"{?x (book:|!book:)|^(book:|!book:)* ?y}\"\n",
    "if len(properties)>0:\n",
    "    p_triple = \"{?x book:\"+properties[0]+\"|^book:\"+properties[0]+\" ?y}\"\n",
    "\n",
    "    \n",
    "if  len(individuals)>0:\n",
    "    first_key = next(iter(individuals))\n",
    "    if(individuals[first_key]==\"not_mapped\"):\n",
    "        if(len(properties)==0):\n",
    "            i_triple = \"{?x a ?y} {?x book:name ?name} FILTER(?name='\"+first_key+\"')\"            \n",
    "        else:\n",
    "            i_triple = \" {?x book:name ?name} FILTER(?name='\"+first_key+\"')\"\n",
    "    else:\n",
    "         i_triple = \"{?x a book:\"+individuals[first_key]+\"} {?x book:name ?name} FILTER(?name='\"+first_key+\"')\"\n",
    "            \n",
    "query_start =      \"\"\"\n",
    "                PREFIX book:<http://www.book-discovery.com/ontologies#>\n",
    "                SELECT  ?y\n",
    "                WHERE{\n",
    "                \"\"\"\n",
    "query_end =    \"}\"\n",
    "\n",
    "query = query_start+c_triple+p_triple+i_triple+query_end\n",
    "print(query)\n",
    "sparql.setQuery(query)\n",
    "result = sparql.query().bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "439c9c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.book-discovery.com/ontologies#life_of_pi\n",
      "http://www.book-discovery.com/ontologies#400_days\n",
      "http://www.book-discovery.com/ontologies#sacred_games\n"
     ]
    }
   ],
   "source": [
    "for x in result:\n",
    "    print(x[\"y\"].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7ed03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7724bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
